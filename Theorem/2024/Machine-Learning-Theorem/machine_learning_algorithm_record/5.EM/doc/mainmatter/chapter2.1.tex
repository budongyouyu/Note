\chapter{算法收敛性}

\section{直观推导}
我们前面已经分别从Jensen不等式和KL散度两个视角推出了EM算法的形式，总体来说就是希望$\theta^{(i)}\rightarrow \theta^{(i+1)}$时，有
\begin{equation}
    logP(X|\theta^{(i)})\leqslant logP(X|\theta^{(i+1)})
\end{equation}

根据前面的推导，我们最优化的方法是最大化证据下界
\begin{equation}
    ELBO=\underbrace{\int_{Z}P(Z|X,\theta^{(i)})logP(X,Z|\theta)dZ}_{\mathcal{L}(\theta,\theta^{^{(i)}})}-\underbrace{\int_{Z}P(Z|X,\theta^{(i)})logP(Z|X,\theta)dZ}_{\mathcal{H}(\theta,\theta^{^{(i)}})}
\end{equation}

我们显然可以得到$\mathcal{L}(\theta^{(i+1)},\theta^{(i)})\geqslant \mathcal{L}(\theta^{(i+1)},\theta^{(i)}$，如果我们想知道每次迭代是否$ELBO$都在增加，只要
\begin{equation}
    \mathcal{H}(\theta^{i+1},\theta^{^{(i)}})\leqslant\mathcal{H}(\theta^{i},\theta^{^{(i)}})
\end{equation}

两者做差很容易证明
\begin{equation}
    \mathcal{H}(\theta^{i+1},\theta^{^{(i)}})-\mathcal{H}(\theta^{i},\theta^{^{(i)}})\leqslant 0
\end{equation}

\section{收敛性定理}

\begin{framed}
    \begin{theorem}
        设$P(Y|\theta)$为观测数据的似然函数，$\theta^{(i)}(i=1,2,\cdots)$为EM算法得到参数估计序列，$P(Y|\theta^{(i)})(i=1,2,\cdots)$为对应的似然函数序列，
        则$P(Y|\theta^{(i)})$是单调递增的，即
        \begin{equation}
            P(Y|\theta^{(i+1)})\geqslant P(Y|\theta^{(i)})
        \end{equation}
    \end{theorem}
\end{framed}

\textbf{proof.}

$\Box$

\begin{framed}
    \begin{theorem}
        设$L(\theta)=\log\ P(Y|\theta)$为观测数据的对数似然函数，$\theta^{(i)}(i=1,2,\cdots)$为$EM$算法对参数估计序列，$L(\theta^{(i)})(i=1,2,\cdots)$对应的对数似然函数序列
        \begin{enumerate}[itemindent=2em]
            \item[(1)] 如果$P(Y|\theta)$有上界，则$L(\theta^{(i)})=\log\ P(Y|\theta^{(i)})$收敛到某一个值$L^*$;
            \item[(2)] 在函数$Q(\theta,\theta^*)$与$L(\theta)$满足一定条件下，由$EM$算法得到的参数估计序列$\theta^{(i)}$的收敛值$\theta^*$是$L(\theta)$的稳定点。
        \end{enumerate}
    \end{theorem}
\end{framed}

\textbf{proof.}
\chapter{从KL散度的视角看EM算法}

\section{主要过程}
我们要最大化对数似然函数
\begin{equation}
    L(\theta)=log\ P(X|\theta)
\end{equation}

其中概率$P(X|\theta)$可以写成
\begin{equation}
    P(X|\theta)=\frac{P(X,Z|\theta)}{P(Z|X,\theta)}
\end{equation}

其中$Z$是隐变量。引入隐变量的概率分布$Q(Z)$


\begin{equation}
    P(X|\theta)=\frac{P(X,Z|\theta)/Q(Z)}{P(Z|X,\theta)/Q(Z)}
\end{equation}

所以

\begin{equation}
    logP(X|\theta)=log\frac{P(X,Z|\theta)}{Q(Z)}-log\frac{P(Z|X,\theta)}{Q(Z)}
\end{equation}

两边对$Q(Z)$求数学期望
\begin{equation}
    \int_{Z}Q(Z)logP(X|\theta)dZ=\int_{Z}Q(Z)log\frac{P(X,Z|\theta)}{Q(Z)}dZ+\int_{Z}Q(Z)log\frac{Q(Z)}{P(Z|X,\theta)}dZ
\end{equation}

分别看式子两边，式子左边$P(X|\theta)$和$Q(Z)$无关，因此
\begin{equation}
    \mbox{\textsl{左边}}=logP(X|\theta)\int_{Z}Q(Z)dZ=logP(X|\theta)=L(\theta)
\end{equation}

式子右边第二项刚好就是$X$和$\theta$下$Z$的分布和$Z$的理想分步下的KL散度，即
\begin{equation}
    KL(Q(Z)||P(Z|X,\theta))=\int_{Z}Q(Z)log\frac{Q(Z)}{P(Z|X,\theta)}dZ
\end{equation}

式子第一项就是证据下界\textsl{ELBO}
\begin{equation}
    \begin{aligned}
        ELBO=\int_{Z}Q(Z)log\frac{P(X,Z|\theta)}{Q(Z)}dZ
    \end{aligned}
\end{equation}

由于$KL$散度正定性，有下面的不等关系
\begin{equation}
    L(\theta)=ELBO+KL(Q(Z)||P(Z|X,\theta))\geqslant ELBO
\end{equation}

等号成立的关键是$KL(Q(Z)||P(Z|X,\theta))=0$，即$Q(Z)=P(Z|X,\theta)$。

\begin{equation}
    \begin{aligned}
        \hat{\theta} &= arg\max_{\theta}\int_{Z}Q(Z)log\frac{P(X,Z|\theta)}{Q(Z)}dZ
    \end{aligned}
\end{equation}

因此$EM$算法就是最大化证据下界$ELBO$，因为$Z$是隐变量无法被观测，所以这里$Q(Z)$的分布我们还没确定，但我们可以假设其等于给定$X$和$\theta$的后验
\begin{equation}
    Q(Z)=P(Z|X,\theta^{(i)})
\end{equation}

其中$\theta^{(i)}$是确定的，我们用来估计$\hat{\theta}=\theta^{(i+1)}$那么
\begin{equation}
    \begin{aligned}
        \theta^{(i+1)} &= arg\max_{\theta}\int_{Z}P(Z|X,\theta^{(i)})log\frac{P(X,Z|\theta)}{P(Z|X,\theta^{(i)})}dZ
    \end{aligned}
\end{equation}

这样EM算法就是一个迭代的算法，利用前次计算的参数来推断下一次。
\begin{equation}
    \begin{aligned}
        \theta^{(i+1)} &= arg\max_{\theta}\int_{Z}P(Z|X,\theta^{(i)})log\frac{P(X,Z|\theta)}{P(Z|X,\theta^{(i)})}dZ\\
        &=arg\max_{\theta}\int_{Z}P(Z|X,\theta^{(i)})logP(X,Z|\theta)dZ-\int_{Z}P(Z|X,\theta^{(i)})logP(Z|X,\theta^{(i)})dZ\\
        &= arg\max_{\theta}\int_{Z}P(Z|X,\theta^{(i)})logP(X,Z|\theta)dZ
    \end{aligned}
\end{equation}

\section{KL散度正定性证明}

我们要证明KL散度的正定性，即
\begin{equation}
    KL(p(z)||q(z))=\int_{z}p(z)log\frac{p(z)}{q(z)}dz\geqslant 0
\end{equation}

注意到$log$函数是一个\textsl{凹函数}\footnote{若$f$是凹函数，则$\sum_{i} k_if(x_i)\leqslant f(\sum_{i} k_ix_i)$}，
所以
\begin{equation}
    \begin{aligned}
        \int_{z} p(z)log\frac{p(z)}{q(z)}dz&=-\int_{z} p(z)log\frac{q(z)}{p(z)}dz\\
        &\leqslant -log\int_{z} q(z)dz=-log\ 1=0
    \end{aligned}
\end{equation}

因此
\begin{equation}
    KL(p(z)||q(z))=\int_{z}p(z)log\frac{p(z)}{q(z)}dz\geqslant 0
\end{equation}



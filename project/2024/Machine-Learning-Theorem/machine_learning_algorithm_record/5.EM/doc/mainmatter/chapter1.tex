\chapter{EM算法}

\section{背景}

EM算法主要是为了解决生成模型参数$\theta$不均匀问题，也就是含有隐变量模型的learning问题$\hat{\theta}=arg\max\limits_{\theta}P(X|\theta)$。

\begin{example}
    假设有三枚硬币，分别记作$A,B,C$，这些硬币正面出现的概率分别是$\pi$、$p$、$q$，进行如下试验，先投掷硬币$A$，根据
    其结果选出硬币$B$或硬币$C$，正面选$B$，反面选$C$，然后掷选出的硬币，投掷硬币的结果，正面记作1，反面记作0，独立重复$n=10$
    次试验，结果如下
    \begin{equation}
        1,1,0,1,0,0,1,0,1,1
    \end{equation}

    假设只能观测到投掷硬币的结果，不能观测到过程，问如何估计三硬币正面出现的概率，即三硬币模型的参数。
\end{example}

\textbf{Solution. }

概率模型写作
\begin{equation}
    \begin{aligned}
         P(y|\theta)&=\sum_{z}P(y,z|\theta)=\sum_{z}P(z|\theta)P(y|z,\theta)\\
        &=\pi p^{y}(1-p)^{1-y}+(1-\pi)q^{y}(1-q)^{1-y}
    \end{aligned}
\end{equation}

这里$y$是观测变量，$z$是\textbf{隐变量}，表示$A$投掷的结果，$\theta=(\pi,p,q)$表示模型参数。

将观测数据表示为$Y=(Y_1,Y_2,\cdots,Y_n)^T$，未观测数据表示为$Z=(Z_1,Z_2,\cdots,Z_n)^T$，则观测数据的似然函数为
\begin{equation}
    \begin{aligned}
        P(Y|\theta)&=\sum_{Z}P(Z|\theta)P(y|Z,\theta)\\
       &=\prod_{j=1}^{n}\pi p^{y_j}(1-p)^{1-y_j}+(1-\pi)q^{y_j}(1-q)^{1-y_j}
   \end{aligned}
\end{equation}

考虑求模型参数$\theta=(\pi,p,q)$的极大似然估计
\begin{equation}
    \hat{\theta}=\arg \max\limits_{\theta}\ log\ P(Y|\theta)
\end{equation}

$\Box$

\begin{question}
    为什么这个问题没法儿求解。
\end{question}

这个问题是没有解析解的，只有通过迭代的算法求解，EM算法就是求解这个问题的一种迭代算法。

\section{EM算法导出}

EM算法的基本问题时近似实现对观测数据的极大似然估计，即面对一个含有隐变量的模型，目标是最大观测数据$Y$关于$\theta$的对数似然函数。
\begin{equation}
    \begin{aligned}
        L(\theta)&=\log\ P(Y|\theta)=\log\sum_{Z}\ P(Y,Z|\theta)\\
        &=log\ \left(\sum_{Z}\ P(Y|Z,\theta)P(Z|\theta)\right)
    \end{aligned}
\end{equation}

$EM$算法是通过迭代逐步近似极大化$L(\theta)$的，假设第$i$次迭代后$\theta$的估计值是$\theta^{(i)}$，我们希望新的估计值能使得$L(\theta)$增加，即$L(\theta)>L(\theta^{(i)})$，
并逐步达到极大值，所以考虑两者的差，注意到$log$函数是凹函数，利用凹函数的\textsl{Jensen不等式}。\footnote{凹函数的Jensen不等式：$f(\sum_{i}\ k_ix_i)\geqslant \sum_{i}k_if(x_i)$}
\begin{equation}
    \begin{aligned}
        L(\theta)-L(\theta^{(i)})&=\log\left(\sum_{Z}P(Z|Y,\theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}\right)-\log P(Y|\theta^{(i)})\\
        &\geqslant \sum_{Z}P(Z|Y,\theta^{(i)})\log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-\log P(Y|\theta^{(i)})\\
        & = \sum_{Z}P(Z|Y,\theta^{(i)})\log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}
    \end{aligned}
\end{equation}

令
\begin{equation}
    B(\theta,\theta^{(i)}) = L(\theta^{(i)}) + \sum_{Z}P(Z|Y,\theta^{(i)})\log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}
\end{equation}

则
\begin{equation}
    L(\theta)\geqslant B(\theta,\theta^{(i)})
\end{equation}

且容易知道
\begin{equation}
    L(\theta^{(i)})= B(\theta^{(i)},\theta^{(i)})
\end{equation}

即$B(\theta,\theta^{(i)})$是$L(\theta)$的一个下界，通过使得下界$B(\theta,\theta^{(i)})$增大使得似然函数增大，这就是EM算法的思路。选择参数$\theta^{(i+1)}$使得
\begin{equation}
    \theta^{(i+1)}=\arg \max_{\theta}\ B(\theta,\theta^{(i)})
\end{equation}

现在求这个参数

\begin{equation}
    \begin{aligned}
        \theta^{(i+1)}&=\arg \max_{\theta}\left(L(\theta^{(i)}) + \sum_{Z}P(Z|Y,\theta^{(i)})\log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}\right)\\
        &=\arg \max_{\theta}\left(\sum_{Z}P(Z|Y,\theta^{(i)})\log P(Y|Z,\theta)P(Z|\theta)\right)\\
        &=\arg \max_{\theta}\left(\sum_{Z}P(Z|Y,\theta^{(i)})\log P(Y,Z|\theta)\right)\\
        &=\arg \max_{\theta}\ Q(\theta,\theta^{(i)})
    \end{aligned}
\end{equation}

即通过求$Q$函数及其极大化，$EM$算法是通过不断求解下界的极大化来逼近求解对数似然函数的算法。

\begin{framed}
    \begin{define}
        (\textbf{Q函数})\ \ 完全数据的对数似然函数$\log\ P(Y,Z|\theta)$关于在给定观测数据$Y$和当前参数$\theta^{(i)}$下对未观测数据$Z$对条件概率分布$P(Z|Y,\theta^{(i)})$对期望称为$Q$函数，即
        \begin{eqnarray}
            Q(\theta,\theta^{(i)})=E_Z\left[\log\ P(Y,Z|\theta)\ |\ Y,\theta^{(i)}\right]
        \end{eqnarray}
    \end{define}
\end{framed}

\section{EM算法}

\begin{framed}
    \textbf{算法(EM算法)}

    输入：观测变量数据$Y$，隐变量数据$Z$，联合分布$P(Y,Z|\theta)$，条件分布$P(Z|Y,\theta)$；
    
    输出：模型参数$\theta$;

    （1）选择参数的初值$\theta^{(i)}$，开始迭代；

    （2）\textbf{E-step}：记$\theta^{(i)}$为第$i$次迭代参数的估计值，在第$i+1$次迭代的$E$步，计算
    \begin{equation}
        \begin{aligned}
            Q(\theta,\theta^{(i)})&=E_Z\left[\log\ P(Y,Z|\theta)\ |\ Y,\theta^{(i)}\right]\\
            &=\sum_{Z}\ log\ P(Y,Z|\theta)P(Z|Y,\theta^{(i)})         
        \end{aligned}
    \end{equation}

    （3）\textbf{M-step}：求$Q(\theta,\theta^{(i)})$极大化$\theta$，确定$i+1$次迭代的参数估计值$\theta^{(i+1)}$
    \begin{equation}
        \theta^{(i+1)}=\arg\ \max_{\theta}Q(\theta,\theta^{(i)})
    \end{equation}

    （4）重复$(2)$和$(3)$，直到收敛；

\end{framed}

值得注意的是，算法参数的初值是任意选择，但是EM算法对初值是敏感的。




